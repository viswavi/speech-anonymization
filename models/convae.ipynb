{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# [ BATCH_SIZE x NUM_TIMESTAMPS x MFCC_FEATURE_DIM ]\n",
    "# need to swap to\n",
    "# [ BATCH_SIZE x MFCC_FEATURE_DIM x NUM_TIMESTAMPS ]\n",
    "\n",
    "class GradReverse(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    Extension of grad reverse layer\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(self, inputs):\n",
    "        return inputs\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input = -grad_input\n",
    "        return grad_input\n",
    "\n",
    "class SexClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SexClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(1024, 512)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, input, constant):\n",
    "        input = GradReverse.grad_reverse(input, constant)\n",
    "        logits = F.relu(self.fc1(input))\n",
    "        logits = F.log_softmax(self.fc2(logits), 1)\n",
    "        return logits\n",
    "\n",
    "# Gated Linear Units\n",
    "class GLU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GLU, self).__init__()\n",
    "        # Custom Implementation because the Voice Conversion Cycle GAN\n",
    "        # paper assumes GLU won't reduce the dimension of tensor by 2.\n",
    "\n",
    "    def forward(self, input):\n",
    "        return input * torch.sigmoid(input)\n",
    "\n",
    "class PixelShuffle(nn.Module):\n",
    "    def __init__(self, upscale_factor):\n",
    "        super(PixelShuffle, self).__init__()\n",
    "        # Custom Implementation because PyTorch PixelShuffle requires,\n",
    "        # 4D input. Whereas, in this case we have have 3D array\n",
    "        self.upscale_factor = upscale_factor\n",
    "\n",
    "    def forward(self, input):\n",
    "        n = input.shape[0]\n",
    "        c_out = input.shape[1] // 2\n",
    "        w_new = input.shape[2] * 2\n",
    "        return input.view(n, c_out, w_new)\n",
    "\n",
    "# define the NN architecture\n",
    "class ConvAutoencoder(nn.Module):\n",
    "    def __init__(self, mfcc_feature_dim):\n",
    "        super(ConvAutoencoder, self).__init__()\n",
    "        \n",
    "        ## model parameters ##\n",
    "        self.mfcc_feature_dim = mfcc_feature_dim\n",
    "\n",
    "        ## encoder layers ##\n",
    "        self.encoder=nn.Sequential(\n",
    "            nn.Conv1d(in_channels=self.mfcc_feature_dim, out_channels=128, kernel_size=15, stride=1, padding=7),\n",
    "            GLU(),\n",
    "            nn.Conv1d(in_channels=128, out_channels=256, kernel_size=5, stride=2, padding=1),\n",
    "            nn.InstanceNorm1d(num_features=256, affine=True),\n",
    "            GLU(),\n",
    "            nn.Conv1d(in_channels=256, out_channels=512, kernel_size=5, stride=2, padding=2),\n",
    "            nn.InstanceNorm1d(num_features=512, affine=True),\n",
    "            GLU()\n",
    "        )\n",
    "\n",
    "        ## decoder layers ##\n",
    "        self.decoder=nn.Sequential(\n",
    "            nn.Conv1d(in_channels=512, out_channels=1024, kernel_size=5, stride=1, padding=2),\n",
    "            PixelShuffle(upscale_factor=2), \n",
    "            nn.InstanceNorm1d(num_features=1024 // 2, affine=True),\n",
    "            GLU(),\n",
    "            nn.Conv1d(in_channels=1024 // 2, out_channels=512, kernel_size=5, stride=1, padding=2),\n",
    "            PixelShuffle(upscale_factor=2), \n",
    "            nn.InstanceNorm1d(num_features=512 // 2, affine=True),\n",
    "            GLU(),\n",
    "            nn.Conv1d(in_channels=512 // 2, out_channels=self.mfcc_feature_dim, kernel_size=15, stride=1, padding=7),\n",
    "        )\n",
    "\n",
    "        ## Sex classifier: num_classes = 2 ##\n",
    "        self.sex_classifier = SexClassifier(2)\n",
    "\n",
    "\n",
    "    def forward(self, input, constant):\n",
    "        ## encode ##\n",
    "        input = self.encoder(input)\n",
    "\n",
    "        ## statistics pooling ##\n",
    "        mean = torch.mean(input, 2)\n",
    "        std = torch.std(input, 2)\n",
    "        stat_pooling = torch.cat((mean, std), 1)\n",
    "\n",
    "        ## sex classifier ##\n",
    "        sex_classifier_logits = self.sex_classifier(stat_pooling, constant)\n",
    "        \n",
    "        ## decode ##\n",
    "        input = self.decoder(input)\n",
    "\n",
    "        ## return reconstructed speech feature for reconstruction loss, sex classification for cross entropy loss ##\n",
    "        return input, sex_classifier_logits\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "# [ BATCH_SIZE x NUM_TIMESTAMPS x MFCC_FEATURE_DIM ]\n",
    "# need to swap to\n",
    "# [ BATCH_SIZE x MFCC_FEATURE_DIM x NUM_TIMESTAMPS ]\n",
    "\n",
    "input = torch.rand(40, 200, 20)\n",
    "input = input.view(40, 20, 200)\n",
    "model = ConvAutoencoder(20)\n",
    "\n",
    "output, sex_logits = model(input, 1)\n",
    "\n",
    "print(output.shape)\n",
    "print(sex_logits.shape)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([40, 20, 200])\n",
      "torch.Size([40, 2])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import argparse\n",
    "from tqdm import tqdm \n",
    "from tqdm import trange\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def main():\n",
    "    # Instantiate the parser\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--lr', type=float, default=1e-6, help='learning rate')\n",
    "    parser.add_argument('--epochs', type=int, default=20, help='number of epochs')\n",
    "    parser.add_argument('--r_weight', type=float, default=0.5, help='reconstruction loss weight')\n",
    "    parser.add_argument('--s_weight', type=float, default=0.5, help='sex classification loss weight')\n",
    "    #parser.add_argument('--a_weight', type=float, default=0.5, help='asr loss weight')\n",
    "    parser.add_argument('--feature_dim', type=int, default=20, help='input feature dim')\n",
    "    parser.add_argument('--output_model_file', type=str, default=\"/checkpoints/model.bin\", help='output path for model')\n",
    "    parser.add_argument('--patience', type=int, default=2)\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "\n",
    "    args.reconstruction_loss = nn.L1Loss(reduction='mean')\n",
    "    args.sex_classification_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    model = ConvAutoencoder(args.feature_dim)\n",
    "\n",
    "    ## train the model ##\n",
    "    train(model, train_loader, dev_loader, args, device)\n",
    "\n",
    "\n",
    "\n",
    "def train(model, train_loader, dev_loader, args, device):\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "    train_losses = []\n",
    "    loss_history = []\n",
    "    no_improvement = 0\n",
    "    for _ in trange(args.epochs, desc=\"Epoch\"):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        nb_tr_examples, nb_tr_steps = 0, 0\n",
    "        for step, batch in enumerate(tqdm(train_loader, desc=\"Training iteration\")):\n",
    "\n",
    "            # TODO: NEED TO CHANGE THIS ACCORDING TO DATA LOADER\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            speech, transcription, sex_label = batch\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward\n",
    "            reconstructed_speech, sex_logits = model(speech)\n",
    "            recon_loss = args.reconstruction_loss(reconstructed_speech, speech)\n",
    "            sex_loss = args.sex_classification_loss(sex_logits, sex_label)\n",
    "\n",
    "            # backward\n",
    "            recon_loss.backward()\n",
    "            sex_loss.backward()\n",
    "            train_loss += args.r_weight * recon_loss.item() + args.s_weight * sex_loss.item()\n",
    "            optimizer.step()\n",
    "\n",
    "        # print out losses\n",
    "        print(\"Loss history:\", train_losses)\n",
    "        print(\"Train loss:\", train_loss/nb_tr_steps)\n",
    "\n",
    "        dev_loss, _, _ = evaluate(model, dev_loader, device=\"cuda\")\n",
    "        print(\"Dev loss:\", dev_loss)\n",
    "\n",
    "        if len(loss_history) == 0 or dev_loss < min(loss_history):\n",
    "            no_improvement = 0\n",
    "            model_to_save = model.module if hasattr(model, 'module') else model\n",
    "            torch.save(model_to_save.state_dict(), args.output_model_file)\n",
    "        else:\n",
    "            no_improvement += 1\n",
    "        \n",
    "        if no_improvement >= args.patience:\n",
    "            print(\"No improvement on development set. Finish training.\")\n",
    "            break\n",
    "\n",
    "        train_losses.append(train_loss/ len(train_loader))\n",
    "        loss_history.append(dev_loss)\n",
    "        \n",
    "\n",
    "def evaluate(model, dev_loader, args, device):\n",
    "    model.eval()\n",
    "    \n",
    "    eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "    predicted_labels, correct_labels = [], []\n",
    "\n",
    "    model.to(device)\n",
    "    for step, batch in enumerate(tqdm(dev_loader, desc=\"Evaluation iteration\")):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        speech, transcription, sex_label = batch\n",
    "\n",
    "        with torch.no_grad():\n",
    "            reconstructed_speech, sex_logits = model(speech)\n",
    "\n",
    "        recon_loss = args.reconstruction_loss(reconstructed_speech, speech)\n",
    "        sex_loss = args.sex_classification_loss(sex_logits, sex_label)\n",
    "\n",
    "        outputs = np.argmax(sex_logits.to('cpu'), axis=1)\n",
    "        sex_label = sex_label.to('cpu').numpy()\n",
    "        \n",
    "        predicted_labels += list(outputs)\n",
    "        correct_labels += list(sex_label)\n",
    "        \n",
    "        eval_loss += args.r_weight * recon_loss.item() + args.s_weight * sex_loss.item()\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    \n",
    "    correct_labels = np.array(correct_labels)\n",
    "    predicted_labels = np.array(predicted_labels)\n",
    "    print(\"Accuracy on testset: \"+str(accuracy_score(correct_labels, predicted_labels)))\n",
    "        \n",
    "    return eval_loss, correct_labels, predicted_labels\n",
    "\n",
    "\n",
    "def test(model, test_loader, args, device):\n",
    "    model.eval()\n",
    "\n",
    "    eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "    predicted_labels, correct_labels = [], []\n",
    "    model.to(device)\n",
    "    for step, batch in enumerate(tqdm(test_loader, desc=\"Testing iteration\")):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        speech, transcription, sex_label = batch\n",
    "\n",
    "        with torch.no_grad():\n",
    "            reconstructed_speech, sex_logits = model(speech)\n",
    "\n",
    "        recon_loss = args.reconstruction_loss(reconstructed_speech, speech)\n",
    "\n",
    "        outputs = np.argmax(sex_logits.to('cpu'), axis=1)\n",
    "        sex_label = sex_label.to('cpu').numpy()\n",
    "        \n",
    "        predicted_labels += list(outputs)\n",
    "        correct_labels += list(sex_label)\n",
    "        \n",
    "        eval_loss += recon_loss.item() \n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    \n",
    "    correct_labels = np.array(correct_labels)\n",
    "    predicted_labels = np.array(predicted_labels)\n",
    "    accuracy = accuracy_score(correct_labels, predicted_labels)*100\n",
    "    print(\"Test Accuracy: \"+str(accuracy))\n",
    "        \n",
    "    return eval_loss, accuracy\n",
    "        \n",
    "\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import speechbrain as sb\n",
    "from speechbrain import Brain\n",
    "\n",
    "def main():\n",
    "    # Instantiate the parser\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--lr', type=float, default=1e-6, help='learning rate')\n",
    "    parser.add_argument('--epochs', type=int, default=20, help='number of epochs')\n",
    "    parser.add_argument('--r_weight', type=float, default=0.5, help='reconstruction loss weight')\n",
    "    parser.add_argument('--s_weight', type=float, default=0.5, help='sex classification loss weight')\n",
    "    #parser.add_argument('--a_weight', type=float, default=0.5, help='asr loss weight')\n",
    "    parser.add_argument('--feature_dim', type=int, default=20, help='input feature dim')\n",
    "    parser.add_argument('--output_model_file', type=str, default=\"/checkpoints/model.bin\", help='output path for model')\n",
    "    parser.add_argument('--patience', type=int, default=2)\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    args.reconstruction_loss = nn.L1Loss(reduction='mean')\n",
    "    args.sex_classification_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    model = ConvAutoencoder(args.feature_dim)\n",
    "\n",
    "    brain = ConvAutoEncoderBrain({\"model\": model})\n",
    "\n",
    "\n",
    "class ConvAutoEncoderBrain(Brain):\n",
    "    def compute_forward(self, batch, stage):\n",
    "        \"\"\"Forward computations from the waveform batches to the output probabilities.\"\"\"\n",
    "        batch = batch.to(self.device)\n",
    "        wavs, wav_lens = batch.sig\n",
    "        tokens_bos, _ = batch.tokens_bos\n",
    "\n",
    "        # Add augmentation if specified\n",
    "        if stage == sb.Stage.TRAIN:\n",
    "            if hasattr(self.modules, \"env_corrupt\"):\n",
    "                wavs_noise = self.modules.env_corrupt(wavs, wav_lens)\n",
    "                wavs = torch.cat([wavs, wavs_noise], dim=0)\n",
    "                wav_lens = torch.cat([wav_lens, wav_lens])\n",
    "                tokens_bos = torch.cat([tokens_bos, tokens_bos], dim=0)\n",
    "\n",
    "        # compute features\n",
    "        feats = self.hparams.compute_features(wavs)\n",
    "\n",
    "        return self.modules.model(feats)\n",
    "\n",
    "\n",
    "    def compute_objectives(self, predictions, batch, stage):\n",
    "        \"\"\"Computes the reconstruction loss and l1 norm loss for sex classification given predictions and targets.\"\"\"\n",
    "\n",
    "        reconstructed_speech, sex_logits  = predictions\n",
    "\n",
    "        sex_label = batch.gender\n",
    "        wavs, wav_lens = batch.sig\n",
    "        # compute features\n",
    "        feats = self.hparams.compute_features(wavs)\n",
    "\n",
    "        reconstruction_loss = nn.L1Loss(reduction='mean')\n",
    "        sex_classification_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "        loss = (\n",
    "            self.hparams.ctc_weight * loss_ctc\n",
    "            + (1 - self.hparams.ctc_weight) * loss_seq\n",
    "        )\n",
    "\n",
    "        if stage != sb.Stage.TRAIN:\n",
    "            current_epoch = self.hparams.epoch_counter.current\n",
    "            valid_search_interval = self.hparams.valid_search_interval\n",
    "            if current_epoch % valid_search_interval == 0 or (\n",
    "                stage == sb.Stage.TEST\n",
    "            ):\n",
    "                # Decode token terms to words\n",
    "                predicted_words = [\n",
    "                    tokenizer.decode_ids(utt_seq).split(\" \") for utt_seq in hyps\n",
    "                ]\n",
    "                target_words = [wrd.split(\" \") for wrd in batch.wrd]\n",
    "                self.wer_metric.append(ids, predicted_words, target_words)\n",
    "\n",
    "            # compute the accuracy of the one-step-forward prediction\n",
    "            self.acc_metric.append(p_seq, tokens_eos, tokens_eos_lens)\n",
    "        return torch.nn.functional.l1_loss(predictions, batch[0])\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "brain = ConvAutoEncoderBrain({\"model\": model})"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "0e3f5f97b5f10c809a183d516d506948609ae516828921e52f22d799f6665a67"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}